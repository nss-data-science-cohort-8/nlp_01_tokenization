{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefbf44b",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "In this notebook, you'll see how to use the NLTK library to tokenize and normalize text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0870f",
   "metadata": {},
   "source": [
    "In this notebook, we'll be working with the full text of Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/moby_dick.txt', encoding = 'utf-8') as fi:\n",
    "    moby = fi.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d8e7a",
   "metadata": {},
   "source": [
    "First, let's split into sentences. For this, we can use the `sent_tokenize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d814045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6a68c",
   "metadata": {},
   "source": [
    "If we want to split into tokens, we can utilize one of nltk's word tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000\n",
    "print(sentences[i])\n",
    "word_tokenize(sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579abad2",
   "metadata": {},
   "source": [
    "Notice how `word_tokenize` counts punctuation marks as tokens. If we want to be more specific in what we count as a token, we can make use of the `regexp_tokenize` function which allows us to specify a regular expression pattern to define a token.\n",
    "\n",
    "For example, we can look for word characters using `\\w`. This will match one or more of any letter or digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tokenize(sentences[1000], r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883d7e7",
   "metadata": {},
   "source": [
    "Let's add a couple of other types of characters to catch and then count up the most frequent tokens using the `Counter` class. This will create a dictionary whose keys are the tokens and values are the frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31333acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter = Counter(regexp_tokenize(moby, r'[-\\'\\w]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f05876",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter['Ishmael']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2771cfe",
   "metadata": {},
   "source": [
    "Let's see how large a vocabulary we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfcfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moby_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c47246",
   "metadata": {},
   "source": [
    "Or how many total words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54efe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(moby_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785a2a4",
   "metadata": {},
   "source": [
    "We can also see the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cee733",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40159b",
   "metadata": {},
   "source": [
    "You'll notice that the most common words include a large number of words like \"the\" and \"of\". These can be considered \"stop words\", and in certain applications are less interesting and can be removed.\n",
    "\n",
    "NLTK includes lists of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd61861",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5dd64",
   "metadata": {},
   "source": [
    "Let's make two modifications to our counter above. First, we'll covert all text to lowercase and the we'll remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter = Counter([x.lower() for x in regexp_tokenize(moby, r'[-\\'\\w]+') if x.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1688a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moby_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6816a",
   "metadata": {},
   "source": [
    "We might also want to do further preprocessing of our text. Let's try stemming and lemmatization. First, let's look at stemming. We'll try out the PorterStemmer from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0bf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ece92",
   "metadata": {},
   "source": [
    "To use it, you just need to call the `.stem` method and pass in the token to be stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc018cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem('whales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem('numerical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter = Counter([porter.stem(x.lower()) for x in regexp_tokenize(moby, r'[-\\'\\w]+') if x.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moby_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cce216",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda833a",
   "metadata": {},
   "source": [
    "One disadvanatage of stemming is that you can end up with non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem('remove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b2b7b",
   "metadata": {},
   "source": [
    "We might instead try a lemmatizer, like the WordNetLemmatizer from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl.lemmatize('whales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter = Counter([wnl.lemmatize(x.lower()) for x in regexp_tokenize(moby, r'[-\\'\\w]+') if x.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12996343",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moby_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d099e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88fa07",
   "metadata": {},
   "source": [
    "Finally, let's look at the distribution of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17019757",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.hist(moby_counter.values(), bins = 50, edgecolor = 'black')\n",
    "plt.xlabel('Number of Appearances')\n",
    "plt.ylabel('Number of Words');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(moby_counter.values()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57bf51",
   "metadata": {},
   "source": [
    "If we want a fun way to visualize the most frequent words in the text, we can use a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c4f4e-ae5a-42d0-b012-517db3291083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcceab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud()\n",
    "\n",
    "wc.generate_from_frequencies(moby_counter)\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
